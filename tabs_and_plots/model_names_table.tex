\scalebox{1}{\begin{tabular}{clrrl}
\toprule
& Model & \makecell{Number of\\ parameters\\(in millions)} & \makecell{Training data\\ size (in \\ Gigabytes)} & \makecell{Training data\\ languages} \\
\midrule
\multirow{10}{*}{\rotatebox[origin=c]{90}{Causal}} & BLOOM-7B1 \cite{workshop2022bloom} & 7000 & 1600 & 46 languages including English, French and Spanish \\
 & Falcon-40B \cite{-} & 40000 & - & - \\
 & GPT-J-6B \cite{-} & 6000 & - & - \\
 & LLAMA-70B \cite{-} & 70000 & - & - \\
 & Medalpaca-7B \cite{-} & 7000 & - & - \\
 & Mistral-7B \cite{-} & 7000 & Undisclosed & Undisclosed \\
 & OPT-66B \cite{-} & 66000 & - & - \\
 & Vicuna-13B \cite{-} & 13000 & - & - \\
 & Vicuna-7B \cite{-} & 7000 & - & - \\
 & Vigogne-13B \cite{-} & 13000 & - & - \\
\midrule
\multirow{16}{*}{\rotatebox[origin=c]{90}{Masked}} & BERT-large \cite{-} & 345 & - & English \\
 & BETO \cite{-} & 110 & - & Spanish \\
 & BSC-Bio \cite{-} & 110 & - & Spanish \\
 & BSC-BioEHR \cite{-} & 110 & - & Spanish \\
 & Bio\_ClinicalBERT \cite{-} & 110 & - & English \\
 & CamemBERT-bio \cite{-} & 110 & - & French \\
 & CamemBERT-large \cite{-} & 335 & - & French \\
 & ClinicalBERT \cite{-} & 110 & - & English \\
 & DrBERT-4GB \cite{-} & 4000 & - & French \\
 & FlauBERT-large \cite{-} & 335 & - & French \\
 & MedBERT \cite{-} & 110 & - & English \\
 & PatanaBERT \cite{-} & 110 & - & Spanish \\
 & RoBERTa-large \cite{-} & 355 & 160 & English \\
 & TulioBERT \cite{-} & 110 & - & Spanish \\
 & XLM-RoBERTa-large \cite{-} & 355 & - & 100 languages including English, French and Spanish \\
 & mBERT \cite{devlin2019bert} & 110 & - & 104 languages including English, French and Spanish \\
\bottomrule
\end{tabular}}