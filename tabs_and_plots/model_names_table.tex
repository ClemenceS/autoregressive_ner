\begin{table}[ht]
\centering
\scalebox{0.7}{\begin{tabular}{cllrrl}
\toprule
& \# & Model & \makecell{Number of\\ parameters\\(in billions)} & \makecell{Training data\\ size} & \makecell{Training corpus\\ language(s) and details} \\
\midrule
\multirow{10}{*}{\rotatebox[origin=c]{90}{Causal}} & 1 & LLAMA-2-70B\textsuperscript{\texttt{[en]}} \cite{touvron2023llama} & 70 & 2 trillion tokens & Mainly English \\
 & 2 & Mistral-7B\textsuperscript{\texttt{[?]}} \cite{jiang2023mistral} & 7 & Undisclosed & Undisclosed \\
 & 3 & BLOOM-7B1\textsuperscript{\texttt{[en]}}\textsuperscript{\texttt{[fr]}}\textsuperscript{\texttt{[es]}} \cite{workshop2022bloom} & 7 & 1600 GiB & 46 languages including English, French and Spanish \\
 & 4 & Falcon-40B\textsuperscript{\texttt{[en]}}\textsuperscript{\texttt{[fr]}}\textsuperscript{\texttt{[es]}} & 40 & 1 trillion tokens & Mainly English, French, Spanish and German \\
 & 5 & GPT-J-6B\textsuperscript{\texttt{[en]}} \cite{wang2021gptj} & 6 & 825 GiB & English \\
 & 6 & OPT-66B\textsuperscript{\texttt{[en]}} \cite{zhang2022opt} & 66 & 180 billion tokens & Mainly English \\
 & 7 & Vicuna-13B\textsuperscript{\texttt{[en]}}* \cite{zheng2023judging} & 13 & 125K conversations & LLAMA 2, fine-tuned on conversations collected from ShareGPT.com, mainly in English \\
 & 8 & Vicuna-7B\textsuperscript{\texttt{[en]}}* \cite{zheng2023judging} & 7 & 125K conversations & LLAMA 2, fine-tuned on conversations collected from ShareGPT.com, mainly in English \\
 & 9 & Medalpaca-7B\textsuperscript{\texttt{[en]}}* \cite{han2023medalpaca} & 7 & 400K Q.A. pairs & LLAMA 2, fine-tuned on semi-generated medical question-answer pairs in English \\
 & 10 & Vigogne-13B\textsuperscript{\texttt{[fr]}}\textsuperscript{\texttt{[en]}}* & 13 & 52K instructions & LLAMA 2, fine-tuned on English instructions automatically translated to French \\
\midrule
\multirow{16}{*}{\rotatebox[origin=c]{90}{Masked}} & 11 & mBERT\textsuperscript{\texttt{[en]}}\textsuperscript{\texttt{[fr]}}\textsuperscript{\texttt{[es]}} \cite{devlin2019bert} & 0.11 & Undisclosed & 104 languages including English, French and Spanish \\
 & 12 & XLM-RoBERTa-large\textsuperscript{\texttt{[en]}}\textsuperscript{\texttt{[fr]}}\textsuperscript{\texttt{[es]}} \cite{conneau2020unsupervised} & 0.355 & 2.5 TB & 100 languages including English, French and Spanish \\
 & 13 & BERT-large\textsuperscript{\texttt{[en]}} \cite{devlin2019bert} & 0.345 & 3,3 billion words & English \\
 & 14 & RoBERTa-large\textsuperscript{\texttt{[en]}} \cite{liu2019roberta} & 0.355 & 160 GiB & English \\
 & 15 & Bio\_ClinicalBERT\textsuperscript{\texttt{[en]}} \cite{alsentzer2019publicly} & 0.11 & 2 million clinical notes & English \\
 & 16 & ClinicalBERT\textsuperscript{\texttt{[en]}} \cite{wang2023optimized} & 0.11 & 1.2 billion words & English \\
 & 17 & MedBERT\textsuperscript{\texttt{[en]}} \cite{charangan2022medbert} & 0.11 & 57 million words & English \\
 & 18 & CamemBERT-large\textsuperscript{\texttt{[fr]}} \cite{martin2019camembert} & 0.335 & 64 billion tokens & French \\
 & 19 & FlauBERT-large\textsuperscript{\texttt{[fr]}} \cite{le2019flaubert} & 0.335 & 13 billion tokens & French \\
 & 20 & DrBERT-4GB\textsuperscript{\texttt{[fr]}} \cite{labrak2023drbert} & 0.11 & 1 billion words & French \\
 & 21 & CamemBERT-bio\textsuperscript{\texttt{[fr]}} \cite{touchent2023camembertbio} & 0.11 & 413 million words & French \\
 & 22 & BETO\textsuperscript{\texttt{[es]}} \cite{canete2020beto} & 0.11 & 3 billion words & Spanish \\
 & 23 & PatanaBERT\textsuperscript{\texttt{[es]}} & 0.11 & Undisclosed & Spanish \\
 & 24 & TulioBERT\textsuperscript{\texttt{[es]}} & 0.11 & Undisclosed & Spanish \\
 & 25 & BSC-BioEHR\textsuperscript{\texttt{[es]}} \cite{carrino2022pretrained} & 0.11 & 1.1 billion tokens & Spanish \\
 & 26 & BSC-Bio\textsuperscript{\texttt{[es]}} \cite{carrino2022pretrained} & 0.11 & 963 million tokens & Spanish \\
\bottomrule
\end{tabular}}
\caption{Characterization of the language models used in our experiments in terms of parameters and training corpus. Models marked with \textsuperscript{\texttt{[en]}} (respectively \textsuperscript{\texttt{[fr]}}, \textsuperscript{\texttt{[es]}}) are mainly trained on English (respectively French, Spanish). CLMs marked with * are fine-tuned versions of other CLMs.}
\label{tab:LM_features}
\end{table}
